# 1 对原理的理解

**我的理解：回归的高级高级高级高级版本，思想都是一致的， y=f(x) ，确定函数， 回归要确定线性函数的权重， 这里要确认每一层的参数**







![img](https://upload-images.jianshu.io/upload_images/2119554-04840a3e9ee185f3.png?imageMogr2/auto-orient/strip|imageView2/2/w/364)

深层神经网络由一个输入层，数个隐层，以及一个输出层构成。每层有若干个神经元，神经元之间有连接权重。每个神经元模拟人类的神经细胞，而结点之间的连接模拟神经细胞之间的连接。

深度学习过程就是选择一个最佳函数的过程，也可以说成为函数寻找最佳参数的过程。如下图左所示，深度学习过程主要分为三步：
 　1. 输入一系列的函数作为待训练模型
 　2. 评价各个函数的好坏，使用误差率作为标准
 　3. 通过每个函数的输出与正确的结果对比，来选择最佳的匹配函数。下面分别介绍下每一步所做的工作。

![img](https://upload-images.jianshu.io/upload_images/2119554-9467a4c1bb8e9bee.png?imageMogr2/auto-orient/strip|imageView2/2/w/463)



*深度学习网络与多层神经网络的区别*

从广义上说深度学习的网络结构也是多层神经网络的一种。传统意义上的多层神经网络是只有输入层、隐藏层、输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适。而深度学习中最著名的卷积神经网络CNN，在原来多层神经网络的基础上，加入了特征学习部分，这部分是模仿人脑对信号处理上的分级的。具体操作就是在原来的全连接的层前面加入了部分连接的卷积层与降维层，而且加入的是一个层级。 输入层 - 卷积层 -降维层 -卷积层 - 降维层 – …. – 隐藏层 -输出层简单来说，**原来多层神经网络做的步骤是：特征映射到值。特征是人工挑选。深度学习做的步骤是 信号->特征->值。 特征是由网络自己选择。**



## 1.1   定义一系列的函数作为待训练模型

神经网络中一个神经元就相当于一个简单的线性函数，线性函数的定义如下：



![img](https://upload-images.jianshu.io/upload_images/2119554-133b44f89b532035.png?imageMogr2/auto-orient/strip|imageView2/2/w/269)

　　其中，w1…wk就是作为函数的参数，也是神经元每个输入的权重值，b作为函数的偏移量。神经元的结构如下图所示。途中除了函数的定义中存在的权重值w和偏移量b以外，还存在一个激活函数。

![img](https://upload-images.jianshu.io/upload_images/2119554-72b256c4e7bb01ad.png?imageMogr2/auto-orient/strip|imageView2/2/w/398)



  激活函数是用来引入非线性因素的。网络中仅有线性模型的话，表达能力不够。比如一个多层的线性网络，其表达能力和单层的线性网络是相同的。常见的激活函数有sigmod函数，relu函数和tanh函数。



## 1.2 确定函数好坏的方法

下图所示为使用深度学习来训练给定的一张数字图像所代表的实际数字的例子。给定一张2的数字图像作为输入，输入层为256个像素点，而输出层则为0-9这10个数字对应的softmax值，根据softmax值的大小来选择最优的结果作为输出。

![img](https://upload-images.jianshu.io/upload_images/2119554-633a27f93028b078.png?imageMogr2/auto-orient/strip|imageView2/2/w/371)



保证对于整个样本集作为输入，输出的总误差是尽可能小的，误差可以表示为计算的输出值和真实值之间的距离。

## 1.3 挑选最佳函数的方法

要在一系列的函数中寻找最佳的函数，也就是总的误差最小的函数，就是为这个函数寻找最佳的参数，在神经网络中称为权重值。对于下图８一个拥有三个输入神经元的两层前反馈全连接神经网络来讲，光是第一层就有9个权重值，若输入神经元1000，那光这一层神经网络就有1000000个权重值。

![img](https://upload-images.jianshu.io/upload_images/2119554-0c0715d02556ec88.png?imageMogr2/auto-orient/strip|imageView2/2/w/520)



  <font color="#660000">回归中是简单的以argmax 计算 深度学习采用梯度下降法计算最优参数 </font>

这里假设横坐标为权重值w，w是一个m*1的矩阵。对于单连接网络，m代表神经元的个数，对于全连接网络，m代表神经元个数的平方，纵坐标为总的误差值L，w和L的关系可用一条不规则的曲线表示。寻找最佳w的计算过程如下：
 　1.首先，初始化权重w
 　2.然后,计算每个神经元输出的总的误差值，计算误差值对权重值的偏导　数，也就是下图曲线的斜率。
 　3.如果偏导数小于零，说明曲线处于下降趋势，即随着w的增加，误差在减少，因此这时需要继续增加w的值。
 　4.如果偏导数大于零，说明曲线处于上升趋势，即随着w的增加，误差在增大，因此这时需要减少w的值，
 　5.回到步骤2，重复以上过程，直到偏导数趋近于零，也就是处于说在一定的范围内，误差L相对于w处于最小值状态，此时的w可作为较优的一组参数。

![img](https://upload-images.jianshu.io/upload_images/2119554-226fa2e71501e6af.png?imageMogr2/auto-orient/strip|imageView2/2/w/480)



# 2. 构建一个简单的深度学习模型

1. 定义深度神经网络模型，相当于定义一系列的函数，如下图所示。模型类型为sequential（线性叠加模型），线性叠加模型就是把每一层按照次序叠加起来x，每层之间采用全连接方式。创建的神经网络有三层，add是往model里添加层，输入为28\*28的矩阵，输出结果为1*10的矩阵，前两层使用的激活函数是relu，最后挨着输出层使用的激活函数为softmax



![img](https://upload-images.jianshu.io/upload_images/2119554-c867ff390647f34a.png?imageMogr2/auto-orient/strip|imageView2/2/w/646)

2. 评价各个函数的好坏，计算训练值和真实值之间的误差，如下图所示。这里损失函数使用mse。

![img](https://upload-images.jianshu.io/upload_images/2119554-9d21ca90bee4e11a.png?imageMogr2/auto-orient/strip|imageView2/2/w/586)

3. compile用来在模型编译的时候配置损失函数、优化器以及正确率函数。fit是用来训练一个固定迭代次数的模型。x_train为训练样本集就是手写的数字图片，y_train是训练样本的结果集，这里是指样本集的标签，batch_size为每次训练和梯度更新块的大小，nb_epoch为迭代次数。



![img](https://upload-images.jianshu.io/upload_images/2119554-dcce92307cd11c88.png?imageMogr2/auto-orient/strip|imageView2/2/w/665)

训练好模型后，需要测试模型的准确率，使用evaluate函数，如下图。

![img](https://upload-images.jianshu.io/upload_images/2119554-211a05ad24013633.png?imageMogr2/auto-orient/strip|imageView2/2/w/370)

执行过程如下几张截图，由下图16输出可看到底层调用的是TensorFlow框架，而且训练样本集有60000个训练样本，测试样本集有10000个测试样本。

![img](https://upload-images.jianshu.io/upload_images/2119554-b58a6dd44cbc4a0e.png?imageMogr2/auto-orient/strip|imageView2/2/w/233)

从下图可以看出这个深度学习模型有三层，总共的参数有669706个。如下图所示。

![img](https://upload-images.jianshu.io/upload_images/2119554-fe7149ef24c57f25.png?imageMogr2/auto-orient/strip|imageView2/2/w/407)







# 3. 深度模型各层的说明

https://blog.csdn.net/zfjBIT/article/details/88075569

![查看源图像](http://p0.ifengimg.com/pmop/2017/0908/EDBDC1CFDD885A14822977B6B98CDABAE49409B6_size47_w600_h226.jpeg)



**CNN的主要层是 卷积层（convolution），池化（pooling）以及全连接层（fully-connected）,如上图所示，这些是深度学习模型的主干， 其次为了优化深度学习模型的效果，还有归一化（Normalization）和正则化（regularization）相关的层**



**有参数的层主要包括：**

- 卷积    
  - **通过卷积操作对输入图像进行降维和特征抽取，线性操作，所以要加上激活函数**
  - **无论输入图片多大，卷积层的参数规模都是固定的**
- 全连接   
  - 首先说明：**可以不用全连接层的**。
  - 全连接就是个**矩阵乘法**，相当于一个特征空间变换，可以把前面所有有用的信息提取整合。再加上激活函数的非线性映射，多层全连接层理论上可以模拟任何非线性变换。但缺点也很明显: 无法保持空间结构。**因为空间结构特性被忽略了，所以全连接层不适合用于在方位上找Pattern的任务，比如segmentation。**卷积取的是局部特征，全连接就是把以前的局部特征重新通过权值矩阵组装成完整的图。 因为用到了所有的局部特征，所以叫全连接。
  - 全连接层（fully connected layers，FC）在整个卷积神经网络中起到**“分类器”**的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为h\*w的全局卷积，h和w分别为前层卷积结果的高和宽。
  - 目前由于全连接层**参数冗余**（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能。
  - 最近的论文，你会发现，**FC可以用GAP（Global Average Pooling）的方法代替**。后者的思想就是：用 feature map 直接表示属于某个类的 confidence map，比如有10个类，就在最后输出10个 feature map，每个feature map中的值加起来求平均值，然后把得到的这些平均值直接作为属于某个类别的 confidence value，再输入softmax中分类， 更重要的是实验效果并不比用 FC 差。后者的优势是：1.因为FC的参数众多，这么做就减少了参数的数量（在最近比较火的模型压缩中，这个优势可以很好的压缩模型的大小）。2.因为减少了参数的数量，可以很好的减轻过拟合的发生。
  - 全连接层对模型影响参数就是三个：
    - 全接解层的总层数（长度）
    - 单个全连接层的神经元数（宽度）
    - 激活函数
- 归一化层 （Normalization）
  - 归一化（normalization）和标准化（standardization）都属于特征缩放（feature scaling）的方法，用于**数据预处理**
  - BatchNorm；LayerNorm；InstanceNorm；GroupNorm 
  - 缓解梯度弥散
- 嵌入层 （Embedding）
  - “把正整数（索引）转换为固定大小的稠密向量”
  - 为什么使用嵌入层
    - 独热编码（One-hot encoding）向量是高维且稀疏的。假如说我们在做自然语言处理（NLP）的工作，并且有一个包含 2000 个单词的字典。这意味着当我们使用独热编码时，每个单词由一个含有 2000 个整数的向量来表示，并且其中的 1999 个整数都是 0。在大数据集下这种方法的计算效率是很低的。
    - 每个嵌入向量会在训练神经网络时更新。文章的最上方有几张图片，它们展示了在多维空间之中，词语之间的相似程度，这使得我们能够可视化词语之间的关系。同样，对于任何能够通过使用嵌入层而把它变成向量的东西，我们都能够这么做。
    - 和独热编码中每个词向量的长度相比，使用嵌入矩阵能够让每个词向量的长度大幅缩短。简而言之，我们用一个向量 [.32, .02, .48, .21, .56, .15]来代替了词语“deep”。然而并不是每个词被一个向量所代替，而是由其索引在嵌入矩阵中对应的向量代替。再次强调，在大数据集中，这种方法的计算效率是很高的。同时因为在训练深度神经网络过程中，词向量能够不断地被更新，所以我们同样能够在多维空间中探索各个词语之间的相似程度。通过使用像 [t-SNE ](https://lvdmaaten.github.io/tsne/) 这样的降维技术，我们能够把这些相似程度可视化出来
- ... ...

**无参数的层**：

- 多数的激活层(Sigmoid/ReLU)

- 池化层

  - **通过卷积操作，我们完成了对输入图像的降维和特征抽取，但特征图像的维数还是很高。维数高不仅计算耗时，而且容易导致过拟合。为此引入了下采样技术，也称为 pooling即池化操作。**
  - **池化的做法是进行卷积操作之后对得到的特征图像进行分块，图像被划分成的不相交块，对这些区域用一个值代替，如最大值或平均值，得到池化后的图像。如果采用最大值，叫做 max 池化；如果采用均值，叫做均值池化。除了降低图像尺寸之外，下采样带来的另外一个好处是平移、旋转不变性，因为输出值由图像的一片区域计算得到，对于平移和旋转并不敏感。**
  - 池化层作用：
    - 降维，缩减模型大小，提高计算速度
    - 降低过拟合概率，提升特征提取鲁棒性
    - 对平移和旋转不敏感
  - 均值池化是线性，max池化是非线性。一般情况下 max 池化有更好的效果。但近期一些性能优异的网络模型如ResNet和GoogLeNet等是采用全局均值池化（global average pooling，GAP）取代FC来融合学到的深度特征。

- Dropout 

  - 正则化的手段之一，可以防止过拟合

  - 主动将网络的某一层的权重随机矩阵W的元素随机地变为0，从而减小网络的复杂度。

    或者说在传播过程中随机的让某个神经元以概率p停止工作

- ... ...





# 4. 深度学习各层的代码实现

以keras 的inception_resnet_v2为例

```
modelA = InceptionResNetV2(include_top=False,weights='imagenet',input_shape=(None, None, 3))
x = modelA.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5, seed=seed)(x)
predictions = Dense(nb_classes, activation='softmax')(x)
modelA = Model(inputs=modelA.input, outputs=predictions)

```

`GlobalAveragePooling2D` 是全局平均值池化

 keras可以比较方便的直接调用大部分经典模型



以下是对某个模型进行修改的例子（加入attention)

```
def add_attention(base_model):
    '''
    输入:
        base_model:基础模型
    输出:
        加了attention后的basemodel
    '''
    ################## add attention by mao 2019-6-5 21:00 ##################
    #加attention, 点乘后求和
    def getSum(input_tensor):
        '''
        input_tensor : [None, 49, 2048]
        Note:
            函数里面要用的都得在函数内import！！！！！！
        '''
        import keras.backend as K
        res = K.sum(input_tensor, axis=-2)
        return res

    from keras.layers import multiply, Reshape, RepeatVector, Permute, Lambda, Dense, BatchNormalization
    from keras.layers.pooling import GlobalAveragePooling2D
    from keras.models import Model

    x = base_model.output
    print(x.shape)
    _, H,W,C = x.shape
    H = int(H)
    W = int(W)
    C = int(C)
    x = GlobalAveragePooling2D(name='avg_pool_for_attention')(x)    #[None, 7, 7, 2048] -> [None, 1, 1, 2048]
    x = Dense(H*W, activation='softmax', name='attention_w')(x)     #全连接层，输出系数
    x = Reshape((H*W,))(x)                                          #[None, 1, 49, 1] -> [None, 49]
    x = RepeatVector(C)(x)                                          #[None, 49] -> [None, 2048, 49]
    x = Permute((2,1))(x)                                           #[None, 2048, 49] -> [None, 49, 2048]
    x = Reshape((H, W, C))(x)                                       #[None, 49, 2048] -> [None, 7, 7, 2048]
    x = multiply([base_model.output, x])                            #逐个元素乘积
    base_model = Model(inputs=base_model.input, outputs=x)
    ############################## end of attention #########################

    return base_model
```





# 5. 深度学习各层的具体数学实现









## 卷积层

feather map 是怎么生成的？
输入层：在输入层，如果是灰度图片，那就只有一个feature map；如果是彩色图片，一般就是3个feature map（红绿蓝）。

其它层：层与层之间会有若干个卷积核（kernel）（也称为过滤器,filter），上一层每个feature map跟每个卷积核做卷积，都会产生下一层的一个feature map，有N个卷积核(filter)，下层就会产生N个feather map。

==【因为其实是一个filter 滑过整个feature map ，每滑一步，得到一个数字，全滑完以后得到一个新的feature map， 如果stride=1则新的feature map 大小与上一层完全一样，因为从头滑到尾， stride=2，则为新map 大小为上层的1/2】==





![img](https://pic4.zhimg.com/80/v2-c9b00043ba326451979abda5417bfcdf_1440w.jpg)



**卷积核放在神经网络里，就代表对应的权重（weight)**

**卷积核和图像进行点乘（dot product),** **就代表卷积核里的权重单独对相应位置的Pixel进行作用**

这里我想强调一下点乘，虽说我们称为卷积，实际上是位置一一对应的点乘，不是真正意义的卷积

假设我们已经知道对应分量以及卷积核

![img](https://pic4.zhimg.com/80/v2-e6a6eb874f469ae1f9ce35ac50027303_1440w.jpg)

我们知道输入，知道神经元的权值（weights）了，根据神经网络公式：

$Output =\Sigma\omega_i x_i +b$

我们还需要定义bias, 不过懒得管它了，给它设为零吧，b = 0

于是

我们发生如下过程, 因为卷积核是3x3的

所以我们分别对三个分量的其中一个3x3的九宫格进行卷积

比如我们在分量的中间找一个3x3九宫格

![img](https://pic2.zhimg.com/80/v2-9f990e17f974d3f49236d0c1c8fba2f5_1440w.jpg)

所以，结果为：

$W1output = 1*(-1) +1*1+1*0+0*(-1)+1*0+2*1+0*(-1)+1*1+2*(-1) =1$

$W2output = 2*1+2*0+1*1+1*1+0*1+0*2+0*1+0*0+1*1=5$

$W3output = 1*(-1)+1*(-1)+0*(-1)+0*2+0*(1)+0*2+1*0+1*1+0*1 = -1$

Bias = 0

Final_output =**W1output + W2output+W3output+bias**= 1+5-1+0 = 5

三个卷积核的输出为什么要叠加在一起呢

**你可以理解为三个颜色特征分量叠加成RGB特征分量**



我们在这已经知道卷积可以提取特征

但是我们也不能随机找图像的pixels进行卷积吧

**上一次我们讲到，我们卷积输出的特征图（feature map）,除了特征值本身外，还包含相对位置信息**

比如人脸检测，眼睛，鼻子，嘴巴都是从上到下排列的

那么提取出的相应的特征值也是按照这个顺序排列的

所以，我们卷积的方式也希望按照正确的顺序

因此

我们实现卷积运算最后的方式

先从左到右，再从上到下，直到所有pixels都被卷积核过了一遍，完成输入图片的第一层卷积层的特征提取



## 采样层（POOLING，池化）

卷积输出的特征图（feature map）到了采样（pooling，有些也叫subsample）层手上，

采样层实际上就是一个特征选择的过程

假设我们用边缘滤波器去卷积输入图片，得到的特征值矩阵如下：

![img](https://pic2.zhimg.com/80/v2-c03717a0e283578ab302075c1c1fe029_1440w.jpg)

其实采样层(pooling)非常好理解，我们这里特指maxpooling

什么是maxpooling 呢

实际操作就是在四个方格里选最大的那个，对，就是9

这个矩阵就是特征图

数字的含义，你可以理解为能代表这个特征的程度

比如上一层卷积层的卷积核或者说过滤器是边缘过滤器

9的意思就代表在这个区域，这一块部位最符合边缘特征

**Maxpooling 就是在这个区域内选出最能代表边缘的值，也就是9，然后丢掉那些没多大用的信息**



这里是一个做maxpooling（池化）的例子

![img](https://pic2.zhimg.com/80/v2-16b276fe8c010af144383da29336c1e9_1440w.jpg)

你就可以理解为卷积核每空两格做一次卷积，卷积核的大小是2x2， 但是卷积核的作用是取这个核里面最大的值（即特征最明显的值），而不是做卷积运算

池化层还有什么性质

它**可以一定程度提高空间不变性**，比如说平移不变性，尺度不变性，形变不变性

对，一定程度上

为什么会有空间不变性呢

因为上一层卷积本身就是对图像一个区域一个区域去卷积

因此对于CNN来说

重要是单独区域的特征，以及**特征之间**的相对位置（而不是绝对位置）

图像细微的变换

经过卷积，maxpooling之后，输出结果和原来差别可能不算大，或者没有差别



比如平移不变性（translation invariant）

意思就是图像经过一个小小的平移之后，依然产生相同的池化特征

这个小小的平移要注意了

这个平移是在你的池化矩阵的范围

对于单个像素，有8个变换的方向（上、下、左、右、左上、左下、右上、右下），如果最大层是在2*2的窗口上面实现，这8个可能的配置中，有3个可以准确的产生和平移前相同的结果（filter size 2x2）

![img](https://pic1.zhimg.com/80/v2-d9782397499dbdd6971415078fe1aea4_1440w.jpg)

说明鲁棒性还行



Pooling 层说到底还是一个特征选择，信息过滤的过程，也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，我觉得这个妥协会越来越小。

现在有些网络都开始少用或者不用pooling层了



关于，average pooling

实际上就是把filter 里面的所以值求一个平均值

特征提取的误差主要来自两个方面：

（1）邻域大小受限；

（2）卷积层权值参数误差。

目前主流上对于average pooling 和max-pooling 的主要区别在于

average -pooling能减小第一种误差，更多的保留图像的背景信

max-pooling能减小第二种误差，更多的保留纹理信息



## 激活函数

实际上，从下采样层出来的结果，不是又到卷积层，而是进入了一个激活函数（activation function）

激活函数是为了让神经网络能够解决非线性问题



## 全连接层

https://zhuanlan.zhihu.com/p/33841176

![img](https://pic1.zhimg.com/80/v2-2e9a9de3ce6493bbf9c0a6dead4e3ba8_1440w.jpg)

当我第一次看到这个全连接层，我的第一个问题是：



**它是怎么样把3x3x5的输出，转换成1x4096的形式**

**我们实际就是用一个3x3x5x4096的卷积层去卷积激活函数的输出**

![img](https://pic1.zhimg.com/80/v2-677c85adc52245a0c3bd6c766e057da8_1440w.jpg)

从上图我们可以看出，我们用一个3x3x5的filter 去卷积激活函数的输出，得到的结果就是一个fully connected layer 的一个神经元的输出，这个输出就是一个值【也就是选取4096个3x3x5的filter】



这一步卷积一个非常重要的作用

就是把分布式特征representation映射到样本标记空间

**就是它把特征representation整合到一起，输出为一个值**



**这样做,有一个什么好处？**

**就是大大减少特征位置对分类带来的影响**

**因为空间结构特性被忽略了，所以全连接层不适合用于在方位上找Pattern的任务，比如segmentation**

ok, 我们突然发现上面的全连接层有两层 1x4096 fully connected layer平铺结构

(有些网络结构有一层的，或者二层以上的，取决于网络设计)





![img](https://pic4.zhimg.com/80/v2-c677192a5bf87760b34ea569e95dc1a3_1440w.jpg)



但是大部分是两层以上呢

这是为啥子呢



泰勒公式都知道吧

意思就是用多项式函数去拟合光滑函数

我们这里的全连接层中一层的一个神经元就可以看成一个多项式



我们用许多神经元去拟合数据分布

但是只用一层fully connected layer 有时候没法解决非线性问题

**而如果有两层或以上fully connected layer就可以很好地解决非线性问题了**



**我们都知道，全连接层之前的作用是提取特征**

**全连接层的作用是分类**

我们现在的任务是去区别一图片是不是猫

假设这个神经网络模型已经训练完了

全连接层已经知道



![img](https://pic4.zhimg.com/80/v2-1d26c5beb983bc63f6858e782d87222f_1440w.jpg)

当我们得到以上特征，我就可以判断这个东东是猫了





因为全连接层的作用主要就是实现分类（Classification）

从下图，我们可以看出



![img](https://pic4.zhimg.com/80/v2-ba7629e4fb2996750f870a1d85bca863_1440w.jpg)



**红色的神经元表示这个特征被找到了（激活了）**

**同一层的其他神经元，要么猫的特征不明显，要么没找到**

当我们把这些找到的特征组合在一起，发现最符合要求的是猫

ok，我认为这是猫了

那我们现在往前走一层

**那们现在要对子特征分类，也就是对猫头，猫尾巴，猫腿等进行分类**

比如我们现在要把猫头找出来



![img](https://pic3.zhimg.com/80/v2-db7d81cd42a1c499c5c66e33f1ac48da_1440w.jpg)

猫头有这么些个特征

于是我们下一步的任务

就是把猫头的这么些子特征找到，比如眼睛啊，耳朵啊



![img](https://pic2.zhimg.com/80/v2-671995a238e33a1c4e669340fed561f5_1440w.jpg)

道理和区别猫一样

**当我们找到这些特征，神经元就被激活了（上图红色圆圈）**

这细节特征又是怎么来的？

就是从前面的卷积层，下采样层来的



全连接层参数特多（可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代全连接层来融合学到的深度特征

需要指出的是，用GAP替代FC的网络通常有较好的预测性能

**问题汇总简答（持续更新）：**

**（1）全连接层对模型的影响？**

首先我们明白全连接层的组成如下：

![img](https://pic4.zhimg.com/80/v2-27e053826d9779ef8d8a6faba22a6b7b_1440w.jpg)二层全连接层结构

那么全连接层对模型影响参数就是三个：

1. 全接解层的总层数（长度）
2. 单个全连接层的神经元数（宽度）
3. 激活函数

首先我们要明白激活函数的作用是：

> **增加模型的非线性表达能力**

如果全连接层宽度不变，增加长度：

**优点**：神经元个数增加，模型复杂度提升；全连接层数加深，模型非线性表达能力提高。理论上都可以提高模型的学习能力。

如果全连接层长度不变，增加宽度：

**优点**：神经元个数增加，模型复杂度提升。理论上可以提高模型的学习能力。

难度长度和宽度都是越多越好？

肯定不是

（1）**缺点**：学习能力太好容易造成过拟合。

（2）**缺点**：运算时间增加，效率变低。

那么怎么判断模型学习能力如何？

看Training Curve 以及 Validation Curve，在其他条件理想的情况下，如果Training Accuracy 高， Validation Accuracy 低，也就是过拟合 了，可以尝试去减少层数或者参数。如果Training Accuracy 低，说明模型学的不好，可以尝试增加参数或者层数。至于是增加长度和宽度，这个又要根据实际情况来考虑了。

PS：很多时候我们设计一个网络模型，不光考虑准确率，也常常得在Accuracy/Efficiency 里寻找一个好的平衡点。



## 归一化（Normalization)

https://zhuanlan.zhihu.com/p/35597976

![img](https://pic3.zhimg.com/80/v2-b4c3eb5f007de42d4fdd8bdebff8cf96_1440w.jpg)

大家看上面两只猪，对于人来说，它就是两只一样的猪，只是图片的灰度或者曝光度不一样罢了，于是我们都给它们都标注为社会人

咋一看，好像没毛病

但是，虽然我们人眼看没毛病，可是机器看的方式和我们不一样，他们看的是对应图片的像素值

由于曝光的，灰度等各种原因，他们像素值其实不一样，那么经过卷积层后，他们的特征**很可能不一样**

**于是神经网络就尴尬了，特征都不一样，为啥标签都一样呢？**

**这样，迷惑的神经网络就不知道怎么对权值进行训练了**

![img](https://pic2.zhimg.com/80/v2-d8211ea96bcf72db6160631dae289ab9_1440w.jpg)



## 微调（Finetune）

https://zhuanlan.zhihu.com/p/35890660



![img](https://pic2.zhimg.com/80/v2-e53e6b546fd0b2c31661e54fcfa1ea29_1440w.jpg)

![img](https://pic4.zhimg.com/80/v2-4ccbaed57cc20b7f30a49d583bdc439f_1440w.jpg)

![img](https://pic2.zhimg.com/80/v2-05c97ab1fcaa9c55f9560b38f833bf5d_1440w.jpg)

![img](https://pic1.zhimg.com/80/v2-309bcf3d287cab31aabdeeccaf71b4d0_1440w.jpg)

## 残差网络

https://zhuanlan.zhihu.com/p/72679537

**网络的深度为什么重要**？

我们知道，在CNN网络中，我们输入的是图片的矩阵，也是最基本的特征，整个CNN网络就是一个信息提取的过程，从底层的特征逐渐抽取到高度抽象的特征，网络的层数越多也就意味这能够提取到的不同级别的抽象特征更加丰富，并且**越深的网络提取的特征越抽象，就越具有语义信息**。

**为什么不能简单的增加网络层数**？

对于传统的CNN网络，简单的增加网络的深度，容易**导致梯度消失和爆炸**。针对梯度消失和爆炸的解决方法一般是**正则初始化(**normalized initialization**)**和**中间的正则化层(**intermediate normalization layers**)，**但是这会导致另一个问题，**退化问题**，随着网络层数的增加，在**训练集上的准确率却饱和甚至下降**了。这个和过拟合不一样，因为过拟合在训练集上的表现会更加出色。

在我参考的博客中，作者针对“退化问题”做了实验并得出如下结论：

按照常理更深层的网络结构的解空间是包括浅层的网络结构的解空间的，也就是说深层的网络结构能够得到更优的解，性能会比浅层网络更佳。但是实际上并非如此，深层网络无论从训练误差或是测试误差来看，都有可能比浅层误差更差，这也证明了并非是由于过拟合的原因。导致这个原因可能是因为**随机梯度下降的策略**，往往解到的并不是全局最优解，而是局部最优解，**由于深层网络的结构更加复杂，所以梯度下降算法得到局部最优解的可能性就会更大**。



**如何解决退化问题**

这里提供了一种想法：既然深层网络相比于浅层网络具有退化问题，那么是否可以保留深层网络的深度，又可以有浅层网络的优势去避免退化问题呢？如果将深层网络的后面若干层学习成恒等映射 ![[公式]](https://www.zhihu.com/equation?tex=h%28x%29%3Dx) ，那么模型就退化成浅层网络。但是直接去学习这个恒等映射是很困难的，那么就换一种方式，把网络设计成：

![[公式]](https://www.zhihu.com/equation?tex=H%28x%29%3DF%28x%29%2Bx%5CRightarrow+F%28x%29%3DH%28x%29-x)

只要 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29%3D0) 就构成了一个恒等映射 ![[公式]](https://www.zhihu.com/equation?tex=H%28x%29%3Dx) ，这里 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29) 为残差。

![img](https://pic1.zhimg.com/80/v2-2180e48afdee79382519d1f8a2a7dce8_1440w.jpg)

Resnet提供了两种方式来解决退化问题：identity mapping以及residual mapping。identity mapping指的是图中“弯线”部分，residual mapping指的是非“弯线”的剩余部分。 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29) 是求和前网络映射， ![[公式]](https://www.zhihu.com/equation?tex=H%28x%29) 是输入到求和后的网络映射。

博客中作者举了这样一个例子：假设有个网络参数映射： ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=h%28x%29) ，这里想把5映射成5.1，那么 ![[公式]](https://www.zhihu.com/equation?tex=g%285%29%3D5.1) ，引入残差的映射 ![[公式]](https://www.zhihu.com/equation?tex=H%285%29%3D5.1%3DF%285%29%2B5%2CF%285%29%3D0.1) 。引入残差的映射对输出的变化更加敏感，比如从输出的5.1再变化到5.2时，映射 ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29) 的输出增加了1/51=2%。而残差结构输出的话，映射 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29) 从0.1到0.2，增加了100%。明显后者的输出变化对权重的调整作用更大，所以效果更好。

这种残差学习结构通过前向神经网络+shortcut链接实现，其中shortcut连接相当于简单执行了同等映射，不会产生额外的参数，也不会增加计算复杂度。整个网络依旧可以通过端到端的反向传播训练。



### 5.7.1 残差块

https://zhuanlan.zhihu.com/p/42706477

ResNets是由残差块构建的，首先先解释一下什么是残差块。

一个残差块可以用表示为：

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D%3D+x_l%2B%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B1%7D)

残差块分成两部分直接映射部分和残差部分。 ![[公式]](https://www.zhihu.com/equation?tex=h%28x_l%29) 是直接映射，反应在图1中是左边的曲线； ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29) 是残差部分，一般由两个或者三个卷积操作构成，即图1中右侧包含卷积的部分。

![img](https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_1440w.jpg)

图1中的Weight在卷积网络中是指卷积操作，addition是指单位加操作。

在卷积网络中， ![[公式]](https://www.zhihu.com/equation?tex=x_l) 可能和 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D) 的Feature Map的数量不一样，这时候就需要使用 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积进行升维或者降维（图2）。这时，残差块表示为：

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D%3D+h%28x_l%29%2B%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B2%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=h%28x_l%29+%3D+W%27_lx) 。其中 ![[公式]](https://www.zhihu.com/equation?tex=W%27_l) 是 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积操作，但是实验结果 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积对模型性能提升有限，所以一般是在升维或者降维时才会使用。

![img](https://pic3.zhimg.com/80/v2-54d11fdb5da318615fae5f579f68c31a_1440w.jpg)图2：1*1残差块

一般，这种版本的残差块叫做resnet_v1，keras代码实现如下：

```python
def res_block_v1(x, input_filter, output_filter):
    res_x = Conv2D(kernel_size=(3,3), filters=output_filter, strides=1, padding='same')(x)
    res_x = BatchNormalization()(res_x)
    res_x = Activation('relu')(res_x)
    res_x = Conv2D(kernel_size=(3,3), filters=output_filter, strides=1, padding='same')(res_x)
    res_x = BatchNormalization()(res_x)
    if input_filter == output_filter:
        identity = x
    else: #需要升维或者降维
        identity = Conv2D(kernel_size=(1,1), filters=output_filter, strides=1, padding='same')(x)
    x = keras.layers.add([identity, res_x])
    output = Activation('relu')(x)
    return output
```





### 5.7.2 残差网络

残差网络的搭建分为两步：

1. 使用VGG公式搭建Plain VGG网络
2. 在Plain VGG的卷积网络之间插入Identity Mapping，注意需要升维或者降维的时候加入 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积。

在实现过程中，一般是直接stack残差块的方式。

```python
def resnet_v1(x):
    x = Conv2D(kernel_size=(3,3), filters=16, strides=1, padding='same', activation='relu')(x)
    x = res_block_v1(x, 16, 16)
    x = res_block_v1(x, 16, 32)
    x = Flatten()(x)
    outputs = Dense(10, activation='softmax', kernel_initializer='he_normal')(x)
    return outputs
```

###  5.7.3 为什么叫残差网络

在统计学中，残差和误差是非常容易混淆的两个概念。误差是衡量观测值和真实值之间的差距，残差是指预测值和观测值之间的差距。对于残差网络的命名原因，作者给出的解释是，网络的一层通常可以看做 ![[公式]](https://www.zhihu.com/equation?tex=y%3DH%28x%29) , 而残差网络的一个残差块可以表示为 ![[公式]](https://www.zhihu.com/equation?tex=H%28x%29%3DF%28x%29%2Bx) ，也就是 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29+%3D+H%28x%29-x) ，在单位映射中， ![[公式]](https://www.zhihu.com/equation?tex=y%3Dx) 便是观测值，而 ![[公式]](https://www.zhihu.com/equation?tex=H%28x%29) 是预测值，所以 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29) 便对应着残差，因此叫做残差网络。







### 5.7.4 残差网络的背后原理

残差块一个更通用的表示方式是

![[公式]](https://www.zhihu.com/equation?tex=y_l%3D+h%28x_l%29%2B%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B3%7D)

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+f%28y_l%29%5Ctag%7B4%7D)

现在我们先不考虑升维或者降维的情况，那么在[1]中， ![[公式]](https://www.zhihu.com/equation?tex=h%28%5Ccdot%29) 是直接映射， ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ccdot%29) 是激活函数，一般使用ReLU。我们首先给出两个假设：

- 假设1： ![[公式]](https://www.zhihu.com/equation?tex=h%28%5Ccdot%29) 是直接映射；
- 假设2： ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ccdot%29) 是直接映射。

那么这时候残差块可以表示为：

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+x_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B5%7D)

对于一个更深的层 ![[公式]](https://www.zhihu.com/equation?tex=L) ，其与 ![[公式]](https://www.zhihu.com/equation?tex=l) 层的关系可以表示为

![[公式]](https://www.zhihu.com/equation?tex=x_L+%3D+x_l+%2B+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%5Ctag%7B6%7D)

这个公式反应了残差网络的两个属性：

1. ![[公式]](https://www.zhihu.com/equation?tex=L) 层可以表示为任意一个比它浅的l层和他们之间的残差部分之和；
2. ![[公式]](https://www.zhihu.com/equation?tex=x_L%3D+x_0+%2B+%5Csum_%7Bi%3D0%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29) ， ![[公式]](https://www.zhihu.com/equation?tex=L) 是各个残差块特征的单位累和，而MLP是特征矩阵的累积。

根据BP中使用的导数的链式法则，损失函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon) 关于 ![[公式]](https://www.zhihu.com/equation?tex=x_l) 的梯度可以表示为

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%5Cfrac%7B%5Cpartial+x_L%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%281%2B%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%29+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%2B%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29+%5Ctag%7B7%7D)

上面公式反映了残差网络的两个属性：

1. 在整个训练过程中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29+) 不可能一直为 ![[公式]](https://www.zhihu.com/equation?tex=-1) ，也就是说在残差网络中不会出现梯度消失的问题。
2. ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D) 表示 ![[公式]](https://www.zhihu.com/equation?tex=L) 层的梯度可以直接传递到任何一个比它浅的 ![[公式]](https://www.zhihu.com/equation?tex=l) 层。

通过分析残差网络的正向和反向两个过程，我们发现，当残差块满足上面两个假设时，信息可以非常畅通的在高层和低层之间相互传导，说明这两个假设是让残差网络可以训练深度模型的充分条件。那么这两个假设是必要条件吗？



==【其实就是类似于残差函数y=f(x)+res, 残差网络的相当于用一个简单的直接映射（即f(x)=x)去拟合上一层的数值，然后用卷积去拟合res， 然后主要是fit出res的卷积层的参数】==



###  5.7.5 直接映射是最好的选择

对于假设1，我们采用反证法，假设 ![[公式]](https://www.zhihu.com/equation?tex=h%28x_l%29+%3D+%5Clambda_l+x_l) ，那么这时候，残差块（图3.b）表示为

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+%5Clambda_lx_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B8%7D)

对于更深的L层

![[公式]](https://www.zhihu.com/equation?tex=x_%7BL%7D+%3D+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_i%29x_l+%2B+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%5Ctag%7B9%7D)

为了简化问题，我们只考虑公式的左半部分 ![[公式]](https://www.zhihu.com/equation?tex=x_%7BL%7D+%3D+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_l%29x_l) ，损失函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon) 对 ![[公式]](https://www.zhihu.com/equation?tex=x_l) 求偏微分得

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5Cvarepsilon%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D+%5Cleft%28+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_i%29+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_l%7D+%5Chat%7B%5Cmathcal%7BF%7D%7D%28x_i%2C+%5Cmathcal%7BW%7D_i%29%5Cright%29%5Ctag%7B10%7D+)

上面公式反映了两个属性：

1. 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%3E1) 时，很有可能发生梯度爆炸；
2. 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%3C1) 时，梯度变成0，会阻碍残差网络信息的反向传递，从而影响残差网络的训练。

所以 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 必须等1。同理，其他常见的激活函数都会产生和上面的例子类似的阻碍信息反向传播的问题。

对于其它不影响梯度的 ![[公式]](https://www.zhihu.com/equation?tex=h%28%5Ccdot%29) ，例如LSTM中的门机制（图3.c，图3.d）或者Dropout（图3.f）以及[1]中用于降维的 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积（图3.e）也许会有效果，作者采用了实验的方法进行验证，实验结果见图4

![img](https://pic2.zhimg.com/80/v2-843326b572e2e4c5c8956e289bd3f58d_1440w.jpg)图3：直接映射的变异模型

![img](https://pic4.zhimg.com/80/v2-5d8fd2868a4ba30e61ce477ab00d7f0f_1440w.jpg)图4：变异模型（均为110层）在Cifar10数据集上的表现

从图4的实验结果中我们可以看出，在所有的变异模型中，依旧是直接映射的效果最好。下面我们对图3中的各种变异模型的分析

1. Exclusive Gating：在LSTM的门机制中，绝大多数门的值为0或者1，几乎很难落到0.5附近。当 ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow0) 时，残差块变成只有直接映射组成，阻碍卷积部分特征的传播；当 ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow1) 时，直接映射失效，退化为普通的卷积网络；
2. Short-cut only gating： ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow0) 时，此时网络便是[1]提出的直接映射的残差网络； ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow1) 时，退化为普通卷积网络；
3. Dropout：类似于将直接映射乘以 ![[公式]](https://www.zhihu.com/equation?tex=1-p) ，所以会影响梯度的反向传播；
4. ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) conv： ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积比直接映射拥有更强的表示能力，但是实验效果却不如直接映射，说明该问题更可能是优化问题而非模型容量问题。

所以我们可以得出结论：假设1成立，即

![[公式]](https://www.zhihu.com/equation?tex=y_l+%3D+x_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+w_l%29+%5Ctag%7B11%7D)

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bl%2B1%7D+%3D+x_%7Bl%2B1%7D+%2B+%5Cmathcal%7BF%7D%28x_%7Bl%2B1%7D%2C+w_%7Bl%2B1%7D%29+%3D+f%28y_l%29+%2B+%5Cmathcal%7BF%7D%28f%28y_l%29%2C+w_%7Bl%2B1%7D%29+%5Ctag%7B12%7D)

### 5.7.6  激活函数的位置

[1] 提出的残差块可以详细展开如图5.a，即在卷积之后使用了BN做归一化，然后在和直接映射单位加之后使用了ReLU作为激活函数。

![img](https://pic2.zhimg.com/80/v2-1c02c8b95a7916ad759a98507fb26079_1440w.jpg)图5：激活函数在残差网络中的使用

在2.1节中，我们得出假设“直接映射是最好的选择”，所以我们希望构造一种结构能够满足直接映射，即定义一个新的残差结构 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bf%7D%28%5Ccdot%29) ：

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bl%2B1%7D+%3D+y_l+%2B+%5Cmathcal%7BF%7D%28%5Chat%7Bf%7D%28y_l%29%2C+w_%7Bl%2B1%7D%29+%5Ctag%7B13%7D)

上面公式反应到网络里即将激活函数移到残差部分使用，即图5.c，这种在卷积之后使用激活函数的方法叫做post-activation。然后，作者通过调整ReLU和BN的使用位置得到了几个变种，即5.d中的ReLU-only pre-activation和5.d中的 full pre-activation。作者通过对照试验对比了这几种变异模型，结果见图6。

![img](https://pic4.zhimg.com/80/v2-ffe81dab49de5306fb001e1da7de7ce3_1440w.jpg)图6：基于激活函数位置的变异模型在Cifar10上的实验结果

而实验结果也表明将激活函数移动到残差部分可以提高模型的精度。

该网络一般就在resnet_v2，keras实现如下：

```python
def res_block_v2(x, input_filter, output_filter):
    res_x = BatchNormalization()(x)
    res_x = Activation('relu')(res_x)
    res_x = Conv2D(kernel_size=(3,3), filters=output_filter, strides=1, padding='same')(res_x)
    res_x = BatchNormalization()(res_x)
    res_x = Activation('relu')(res_x)
    res_x = Conv2D(kernel_size=(3,3), filters=output_filter, strides=1, padding='same')(res_x)
    if input_filter == output_filter:
        identity = x
    else: #需要升维或者降维
        identity = Conv2D(kernel_size=(1,1), filters=output_filter, strides=1, padding='same')(x)
    output= keras.layers.add([identity, res_x])
    return output

def resnet_v2(x):
    x = Conv2D(kernel_size=(3,3), filters=16 , strides=1, padding='same', activation='relu')(x)
    x = res_block_v2(x, 16, 16)
    x = res_block_v2(x, 16, 32)
    x = BatchNormalization()(x)
    y = Flatten()(x)
    outputs = Dense(10, activation='softmax', kernel_initializer='he_normal')(y)
    return outputs
```

### 5.7.7 残差网络与模型集成

Andreas Veit等人的论文[3]指出残差网络可以从模型集成的角度理解。如图7所示，对于一个3层的残差网络可以展开成一棵含有8个节点的二叉树，而最终的输出便是这8个节点的集成。而他们的实验也验证了这一点，随机删除残差网络的一些节点网络的性能变化较为平滑，而对于VGG等stack到一起的网络来说，随机删除一些节点后，网络的输出将完全随机。

![img](https://pic2.zhimg.com/80/v2-cbf6ce3da2669335e119a2f222afa6f5_1440w.jpg)图7：残差网络展开成二叉树

### Reference

[1] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.

[2] He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.

[3] Veit A, Wilber M J, Belongie S. Residual networks behave like ensembles of relatively shallow networks[C]//Advances in Neural Information Processing Systems. 2016: 550-558.



